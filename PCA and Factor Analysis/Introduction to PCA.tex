\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Mr. Kevin O'Brien}
\chead{Spring 2013}
%\input{tcilatex}

\begin{document}

\tableofcontents
%http://support.sas.com/publishing/pubcat/chaps/55129.pdf

\section{Introduction to Course}
Advances in data collection and storage capabilities during the past decades have led to an information
overload in most sciences. Researchers working in domains as diverse as engineering, astronomy, biology,
remote sensing, economics, and consumer transactions, face larger and larger observations and simulations
on a daily basis.

Such datasets, in contrast with smaller, more traditional datasets that have been studied
extensively in the past, present new challenges in data analysis. Traditional statistical methods break down
partly because of the increase in the number of observations, but mostly because of the increase in the
number of variables associated with each observation. The dimension of the data is the number of variables
that are measured on each observation.\\(I Fodor, LLNL, USA, June 2002).

\section{Overview of Course}

To ground the students in Applied Multivariate Analysis. This module serves business and mathematics students. It introduces the mathematical statistical ideas behind \begin{itemize} \item Principal Component Analysis, \item Factor Analysis, \item Cluster Analysis, \item Discrimination Function, \item The Multiple Linear Logistic function. \end{itemize}
The students learn how to implement these techniques in SPSS to become competent in the analysis of a wide variety of multivariate data structures.

\section{Data Reduction}

Data Reduction or Dimensionality Reduction pertains to analytic methods (typically multivariate exploratory techniques such as Factor Analysis, Multidimensional Scaling, Cluster Analysis, Canonical Correlation, or Neural Networks) that involve reducing the dimensionality of a data set by extracting a number of underlying factors, dimensions, clusters, etc., that can account for the variability in the (multidimensional) data set.

For example, in poorly designed questionnaires, all responses provided by the participants on a large number of variables (scales, questions, or dimensions) could be explained by a very limited number of "trivial" or artifactual factors. For example, two such underlying factors could be: (1) the respondent's attitude towards the study (positive or negative) and (2) the "social desirability" factor (a response bias representing a tendency to respond in a socially desirable manner).
\newpage
\subsection{Variable Redundancy}
A specific (but fictitious) example of research will now be presented to illustrate the concept of
variable redundancy introduced earlier. Imagine that you have developed a 7-item measure of
job satisfaction. The \emph{instrument} is reproduced here:


\begin{framed}
\begin{verbatim}
Please respond to each of the following statements by placing a
rating in the space to the left of the statement. In making your
ratings, use any number from 1 to 7 in which 1=“strongly disagree”
and 7=“strongly agree.”
_____ 1. My supervisor treats me with consideration.
_____ 2. My supervisor consults me concerning important decisions
that affect my work.
_____ 3. My supervisors give me recognition when I do a good job.
_____ 4. My supervisor gives me the support I need to do my job
well.
_____ 5. My pay is fair.
_____ 6. My pay is appropriate, given the amount of responsibility
that comes with my job.
_____ 7. My pay is comparable to the pay earned by other employees
whose jobs are similar to mine.
\end{verbatim}
\end{framed}

Notice that items 1-4 all deal with the same topic: the employees’ satisfaction with their supervisors. In this way, items 1-4 are somewhat redundant to one another. Similarly, notice that items 5-7 also all seem to deal
with the same topic: the employees’ satisfaction with their pay. Empirical findings may further support the notion that there is redundancy in the seven items. Assume that you administer the questionnaire to 200 employees and compute all possible correlations between responses to the 7 items. The resulting fictitious correlations are
reproduced in the table below.

\begin{figure}[h!]
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.6]{3A.jpg}\\
  \caption{Questionnaire}\label{Correlations}
\end{center}

\end{figure}

When correlations among several variables are computed, they are typically summarized in the
form of a correlation matrix, such as the one reproduced in the previous table. This is an appropriate
opportunity to review just how a correlation matrix is interpreted.

The rows and columns of the table correspond to the seven variables included in the analysis: Row 1 (and column 1)
represents variable 1, row 2 (and column 2) represents variable 2, and so forth. Where a given
row and column intersect, you will find the correlation between the two corresponding variables.
For example, where the row for variable 2 intersects with the column for variable 1, you find a
correlation of 0.75; this means that the correlation between variables 1 and 2 is 0.75.
The correlations show that the seven items seem to hang together in two distinct
groups.

First, notice that items 1-4 show relatively strong correlations with one another. This
could be because items 1-4 are measuring the same construct. In the same way, items 5-7
correlate strongly with one another (a possible indication that they all measure the same
construct as well). Even more interesting, notice that items 1-4 demonstrate very weak
correlations with items 5-7. This is what you would expect to see if items 1-4 and items 5-7
were measuring two different constructs.

Given this apparent redundancy, it is likely that the seven items of the questionnaire are not
really measuring seven different constructs; more likely, items 1-4 are measuring a single
construct that could reasonably be labelled \textbf{\emph{satisfaction with supervision}} while items 5-7 are
measuring a different construct that could be labelled \textbf{\emph{satisfaction with pay}}.

If responses to the seven items actually displayed the redundancy suggested by the pattern of
correlations, it would be advantageous to somehow reduce the number of variables
in this data set, so that (in a sense) items 1-4 are collapsed into a single new variable that reflects
the employees’ satisfaction with supervision, and items 5-7 are collapsed into a single new
variable that reflects satisfaction with pay.

We could then use these two new artificial variables (rather than the seven original variables) as predictor variables in multiple regression, or in any other type of analysis.

In essence, this is what is accomplished by principal component analysis: it allows you to reduce
a set of observed variables into a smaller set of artificial variables called principal components.
The resulting principal components may then be used in subsequent analyses.
\newpage
\subsection{Data Reduction in Exploratory Graphics}

The term can also refer in \textbf{exploratory graphics} to ``Data Reduction by unbiased decreasing of the sample size". This type of Data Reduction is applied in exploratory graphical data analysis of extremely large data sets. The size of the data set can obscure an existing pattern (especially in large line graphs or scatterplots) due to the density of markers or lines. Then, it can be useful to plot only a representative subset of the data (so that the pattern is not hidden by the number of point markers) to reveal the otherwise obscured but still reliable pattern.


\section{Principal Component Analysis }

\subsection{Introduction}
Principal component analysis is appropriate when you have obtained measures on a number of
(possibly) correlated observed variables and wish to develop a smaller number of artificial uncorrelated variables called \textbf{principal components} that will account for most of the variance in the observed variables. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. The principal
components may then be used as predictor or criterion variables in subsequent analyses.


Traditionally, principal component analysis is performed can be performed on raw data, on the symmetric \textbf{Covariance matrix} or on the symmetric \textbf{Correlation matrix}. (The covariance matrix contains scaled sums of squares and cross products. A correlation matrix is like a covariance matrix but first the variables, i.e. the columns, have been standardized.)

If raw data is used, the procedure will create the original correlation matrix or covariance matrix, as specified by the user.  If the correlation matrix is used, the variables are standardized and the total variance will equal the number of variables used in the analysis (because each standardized variable has a variance equal to 1).  If the covariance matrix is used, the variables will remain in their original metric.  However, one must take care to use variables whose variances and scales are similar.  Unlike \textbf{factor analysis}, which analyzes the common variance, the original matrix in a principal components analysis analyzes the total variance.  Also, principal components analysis assumes that each original measure is collected without measurement error.





\subsection{Mathematical background of PCA}
Principal Component Analysis is a linear \textbf{dimensionality reduction} technique, which identifies orthogonal directions of maximum variance in the original data, and projects the data into a lower-dimensionality space formed of a sub-set of the highest-variance components (Bishop, 1995).


The mathematical technique used in PCA is called \textbf{eigen analysis}. Technically, a principal component can be
defined as a linear combination of optimally-weighted observed variables. Software packages compute solutions for these weights by using a special
type of equation called an \textbf{\emph{eigenequation}}. The weights produced by these eigenequations are
optimal weights in the sense that, for a given set of data, no other set of weights could produce a
set of components that are more successful in accounting for variance in the observed variables.
The weights are created so as to satisfy a principle of least squares that is similar (but not identical) to
the principle of least squares used in multiple regression.

Remarks
\begin{itemize}
\item The words \textbf{\emph{linear combination}} refer to the fact that scores on a
component are created by adding together scores on the observed variables being analyzed.
\item \textbf{\emph{Optimally weighted}} refers to the fact that the observed variables are weighted in such a way
that the resulting components account for a maximal amount of variance in the data set.
\end{itemize}

%We solve for the eigenvalues and eigenvectors of a square symmetric matrix with sums of squares and cross products. The eigenvector associated with the largest eigenvalue has the same direction as the first principal component. Similarly the eigenvector associated with the second largest eigenvalue determines the direction of the second principal component, and so on. The sum of the eigenvalues equals the \textbf{trace} of the square matrix and the maximum number of eigenvectors equals the number of rows (or columns) of this matrix.

\subsection{Number of Extracted Components }The preceding section may have created the impression
that, if a principal component analysis were performed on data from the 7-item job satisfaction
questionnaire, only two components would be created.  However, such an impression would not
be entirely correct.

In reality, the number of components extracted in a principal component analysis \textbf{\emph{is equal}} to the
number of observed variables being analyzed.  This means that an analysis of your 7-item
questionnaire would actually result in seven components, not two.

However, in most analyses, only the first few components account for meaningful amounts of
variance, so only these first few components are retained, interpreted, and used in subsequent
analyses (such as in multiple regression analyses).  For example, in your analysis of the 7-item
job satisfaction questionnaire, it is likely that only the first two components would account for a
meaningful amount of variance; therefore only these would be retained for interpretation.  You
would assume that the remaining five components accounted for only trivial amounts of
variance.  These latter components would therefore not be retained, interpreted, or further
analyzed.

\subsection{Characteristics of Principal Components}
The first component extracted in a principal component analysis accounts for a maximal amount of total variance in the observed variables. Under typical conditions, this means that the first component will be correlated with at least
some of the observed variables.  In fact it may be correlated with many.

The second component extracted will have two important characteristics.  First,  this component
will account for a maximal amount of variance in the data set that was not accounted for by the
first component.  Again under typical conditions, this means that the second component will be
correlated with some of the observed variables that did not display strong correlations with
component 1.
The second characteristic of the second component is that it will be uncorrelated with the first
component. If you were to compute the correlation between components 1 and 2, that
correlation should be close to zero.

The remaining components that are extracted in the analysis display the same two characteristics:
each component accounts for a maximal amount of variance in the observed variables that was
not accounted for by the preceding components, and is uncorrelated with all of the preceding
components.  A principal component analysis proceeds in this fashion, with each new component
accounting for progressively smaller and smaller amounts of variance (this is why only the first
few components are usually retained and interpreted).  When the analysis is complete, the
resulting components will display varying degrees of correlation with the observed variables, but
are completely uncorrelated with one another.

\subsection{Total Variance in the context of PCA}
To understand the meaning of \textbf{total
variance} as it is used in a principal component analysis, remember that the observed
variables are standardized in the course of the analysis.  This means that each variable is
transformed so that it has a mean of zero and a variance of one.

The total variance in the
data set is simply the sum of the variances of these observed variables.  Because they have
been standardized to have a variance of one, each observed variable contributes one unit of
variance to the total variance in the data set.  Because of this, the total variance in a
principal component analysis \textbf{\emph{will always be equal}} to the number of observed variables
being analyzed.

For example, if seven variables are being analyzed, the total variance will
equal seven.  The components that are extracted in the analysis will partition this variance:
perhaps the first component will account for 3.2 units of total variance; perhaps the second
component will account for 2.1 units.  The analysis continues in this way until all of the
variance in the data set (i.e. the remaining 1.7 units). has been accounted for.

\subsection{Orthogonal versus Oblique Solutions}

This course will discuss only principal component analysis that result in \textbf{orthogonal solutions}.
An orthogonal solution is one in which the components remain uncorrelated (orthogonal means
“uncorrelated”).

It is possible to perform a principal component analysis that results in correlated components.
Such a solution is called an \textbf{oblique solution}.  In some situations, oblique solutions are superior
to orthogonal solutions because they produce cleaner, more easily-interpreted results.
However, oblique solutions are also somewhat more complicated to interpret, compared to
orthogonal solutions.  For this reason, we will focus only on the interpretation of orthogonal solutions

\subsection{Sampling adequacy (KMO Statistic)}
Measured by the Kaiser-Meyer-Olkin (KMO) statistics, sampling adequacy predicts if the analyses are likely to perform well, based on correlation and partial correlation. KMO can also be used to assess which variables to drop from the model because they are too multi-collinear.

There is a KMO statistic for each individual variable, and their sum is the KMO overall statistic. KMO varies from 0 to 1.0 and KMO overall should be 0.60 or higher to proceed with factor analysis. Values below 0.5 imply that factor analysis or PCA may not be appropriate. (Approach to overcoming this: If it is not, drop the \textbf{indicator variables} with the lowest individual KMO statistic values, until KMO overall rises above 0.60.)

Kaiser-Meyer-OlkinTo compute KMO overall, the numerator is the sum of squared correlations of all variables in the analysis (except the 1.0 self-correlations of variables with themselves, of course). The denominator is this same sum plus the sum of squared partial correlations of each variable i with each variable j, controlling for others in the analysis. The concept is that the partial correlations should not be very large if one is to expect distinct factors to emerge from factor analysis.

\subsection{Bartlett's Test for Sphericity}
Bartlett's measure tests the null hypothesis that the original correlation matrix is an identity
matrix. For PCA and factor analysis to work we need some relationships between variables and if the R-
matrix were an identity matrix then all correlation coefficients would be zero. Therefore, we
want this test to be signifcant (i.e. have a significance value less than 0.05). A significant test
tells us that the correlation matrix is not an identity matrix; therefore, there are some relationships
between the variables we hope to include in the analysis. For these data, Bartlett's test is
highly significant (p < 0.001), and therefore factor analysis is appropriate.


\subsection{Determining the Number of “Meaningful” Components to Retain}
Earlier it was stated that the number of components extracted is equal to the number of variables
being analyzed, necessitating that you decide just how many of these components are truly
meaningful and worthy of being retained for further analysis.

In general, you expect
that only the first few components will account for meaningful amounts of variance, and that the
later components will tend to account for only trivial variance.

The next step of the analysis,therefore, is to determine how many meaningful components should be retained for
interpretation.  The followings section will describe four criteria that may be used in making this decision:
\begin{itemize} \item the eigenvalue-one criterion, \item the scree test, \item the proportion of variance accounted for, \item the
interpretability criterion.
\end{itemize}


\subsection{The Eigenvalue-One Criterion}  In principal component analysis, one of the most commonly
used criteria for solving the number-of-components problem is the eigenvalue-one criterion, also
known as the Kaiser criterion (Kaiser, 1960).  With this approach, you retain and interpret any
component with an eigenvalue greater than 1.00.

The rationale for this criterion is straightforward.  Each observed variable contributes one unit of
variance to the total variance in the data set.  Any component that displays an eigenvalue greater
than 1.00 is accounting  for a greater amount of variance than had been contributed by one
variable.  Such a component is therefore accounting for a meaningful amount of variance, and is
worthy of being retained.

On the other hand, a component with an eigenvalue less than 1.00 is accounting for less variance
than had been contributed by one variable.  The purpose of principal component analysis is to
reduce a number of observed variables into a relatively smaller number of components; this
cannot be effectively achieved if you retain components that account for less variance than had
been contributed by individual variables.  For this reason, components with eigenvalues less than
1.00 are viewed as trivial, and are not retained.


\subsubsection{Advantages and Disadvantages}
The eigenvalue-one criterion has a number of positive features that have contributed to its
popularity.  Perhaps the most important reason for its widespread use is its simplicity:  You do
not make any subjective decisions, but merely retain components with eigenvalues greater than
one.

On the positive side, it has been shown that this criterion very often results in retaining the
correct number of components, particularly when a small to moderate number of variables are
being analyzed and the variable communalities are high.  Stevens (1986) reviews studies that
have investigated the accuracy of the eigenvalue-one criterion, and recommends its use when
less than 30 variables are being analyzed and communalities are greater than .70, or when the
analysis is based on over 250 observations and the mean communality is greater than or equal to
0.60.

There are a number of problems associated with the eigenvalue-one criterion, however.  As was
suggested in the preceding paragraph, it can lead to retaining the wrong number of components
under circumstances that are often encountered in research (e.g., when many variables are
analyzed, when communalities are small).

Also, the mindless application of this criterion can
lead to retaining a certain number of components when the actual difference in the eigenvalues
of successive components is only trivial.  For example, if component 2 displays an eigenvalue of
1.001 and component 3 displays an eigenvalue of 0.999, then component 2 will be retained but
component 3 will not; this may mislead you into believing that the third component was
meaningless when, in fact, it accounted for almost exactly the same amount of variance as the
second component.

In short, the eigenvalue-one criterion can be helpful when used judiciously,
but the thoughtless application of this approach can lead to serious errors of interpretation.

\subsection{The scree test} With the scree test (Cattell, 1966), you plot the eigenvalues associated with
each component and look for a “break” between the components with relatively large
eigenvalues and those with small eigenvalues.  The components that appear before the break are
assumed to be meaningful and are retained for rotation; those apppearing after the break are
assumed to be unimportant and are not retained.

Remark: The word “scree” refers to the loose rubble that lies at
the base of a cliff.  When performing a scree test, you normally hope that the scree plot
will take the form of a cliff:  At the top will be the eigenvalues for the few meaningful
components, followed by a break (the edge of the cliff).  At the bottom of the cliff will lie
the scree:  eigenvalues for the trivial components.

\begin{figure}[h!]
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{3AScree1.jpg}\\
  \caption{Example of a Scree Plot}\label{Scree Plot}
\end{center}

\end{figure}


Sometimes a scree plot will display several large breaks.  When this is the case, you should look
for the last big break before the eigenvalues begin to level off. Only the components that appear
before this last large break should be retained.

\subsubsection{Advantages and Disadvantages}
The scree test can be expected to provide reasonably accurate results, provided the sample is
large (over 200) and most of the variable communalities are large (Stevens, 1986).  However,
this criterion has its own weaknesses as well, most notably the ambiguity that is often displayed
by scree plots under typical research conditions:  Very often, it is difficult to determine exactly
where in the scree plot a break exists, or even if a break exists at all.

\subsection{Proportion of Variance Accounted For}

A third criterion in solving the number of factors
problem involves retaining a component if it accounts for a specified proportion (or percentage)
of variance in the data set.  For example, you may decide to retain any component that accounts
for at least $5\%$ or $10\%$ of the total variance.  This proportion can be calculated with a simple
formula:

\[ \mbox{Proportion}  = \frac{\mbox{Eigenvalue for the component of interest}}{\mbox{Total eigenvalues of the correlation matrix}}  \]

In principal component analysis, the “total eigenvalues of the correlation matrix” is equal to the
total number of variables being analyzed (because each variable contributes one unit of variance
to the analysis).

An alternative criterion is to retain enough components so that the cumulative percent of variance
accounted for is equal to some minimal value.  Suppose that,in a PCA procedure, that components 1, 2, 3,
and 4 accounted for approximately $37\%$, $33\%$, $13\%$, and $10\%$ of the total variance, respectively.

Suppose that it was required to account for $90\%$ of the variance. Adding these percentages together results in a sum of $93\%$.  This means that the cumulative percent of variance accounted for by components 1, 2, 3, and 4 is $93\%$.

\begin{figure}[h!]
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{3AEigen.jpg}\\
  \caption{Eigenvalue Table}\label{Eigenvalue Table}
\end{center}

\end{figure}


\subsubsection{Advantages and Disadvantages}

The proportion of variance criterion has a number of positive features.  For example, in most
cases, you would not want to retain a group of components that, combined, account for only a
minority of the variance in the data set (say, $30\%$).  Nonetheless, many critical values discussed
earlier are obviously arbitrary.  Because of these and related problems, this approach has sometimes been
criticized for its subjectivity (Kim and Mueller, 1978).

\subsection{The Interpretability Criteria}

Perhaps the most important criterion for solving the \textbf{\emph{number of-components}} problem is the interpretability criterion:  interpreting the substantive meaning of the retained components and verifying that this interpretation makes sense in terms of what is known about the constructs under investigation.


The following list provides four rules to follow in doing this.  (A later section shows how to
actually interpret the results of a principal component analysis; the following rules will be more
meaningful after you have completed that section).

\begin{enumerate}
\item Are there at least three variables (items) with significant loadings on each retained
component?  A solution is less satisfactory if a given component is measured by less than
three variables.

\item  Do the variables that load on a given component share the same conceptual meaning?
For example, if three questions on a survey all load on component 1, do all three of these
questions seem to be measuring the same construct?

\item  Do the variables that load on different components seem to be measuring different
constructs?  For example, if three questions load on component 1, and three other
questions load on component 2, do the first three questions seem to be measuring a
construct that is conceptually different from the construct measured by the last three
questions?

\item  Does the rotated factor pattern demonstrate “simple structure?”  Simple structure
means that the pattern possesses two characteristic:

\begin{itemize}
\item[(a)] Most of the variables have
relatively high factor loadings on only one component, and near zero loadings on the other
components, and \item[(b)] most components have relatively high factor loadings for some
variables, and near-zero loadings for the remaining variables.
\end{itemize}
\end{enumerate}
\newpage
%----------------------------------------------------------------------------------%
\section{Review of Important Definitions}
\begin{itemize}
\item An observed variable can be measured directly, is sometimes called a measured variable or an indicator or a
manifest variable.
\item A principal component is a linear combination of weighted observed variables. Principal components are
uncorrelated and orthogonal.
\item A latent construct can be measured indirectly by determining its influence to responses on measured variables. A latent construct could is also referred to as a factor, underlying construct, or unobserved variable.
\item Factor scores are estimates of underlying latent constructs.
\item Unique factors refer to unreliability due to measurement error and variation in the data.
\item Principal component analysis minimizes the sum of the squared perpendicular distances to the axis of the
principal component while least squares regression minimizes the sum of the squared distances perpendicular to the
x axis (not perpendicular to the fitted line).
\item Principal component scores are actual scores.
\item Eigenvectors are the weights in a linear transformation when computing principal component scores.
Eigenvalues indicate the amount of variance explained by each principal component or each factor.
\item Orthogonal means at a 90 degree angle, perpendicular.
Obilque means other than a 90 degree angle.
\item An observed variable \textbf{\emph{loads}} on a factors if it is highly correlated with the factor, has an eigenvector of greater magnitude on that factor.
\item Communality is the variance in observed variables accounted for by a common factors. Communality is more
relevant to EFA than PCA.
\end{itemize}
\newpage
%--------------------------------------------------------------------------------------------------%
\section{Exploratory Factor Analysis}
Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) are both variable reduction techniques
and sometimes mistaken as the same statistical method. However, there are distinct differences between PCA and
EFA. Similarities and differences between PCA and EFA will be examined.


\subsection{Principal Component Analysis (PCA)}
\begin{itemize}
\item Is a variable reduction technique
\item Is used when variables are highly correlated
\item Reduces the number of observed variables to a smaller number of principal components which account for most
of the variance of the observed variables
\item Is a large sample procedure
\end{itemize}

The number of components extracted is equal to the number of observed variables in the analysis. The first principal
component identified accounts for most of the variance in the data. The second component identified accounts for the
second largest amount of variance in the data and is uncorrelated with the first principal component and so on.


The total amount of variance in PCA is equal to the number of observed variables being analyzed. In PCA, observed
variables are standardized, (e.g., mean=0, standard deviation=1).

Components accounting for maximal variance are retained while other components accounting for a trivial amount of
variance are not retained. Eigenvalues indicate the amount of variance explained by each component. Eigenvectors
are the weights used to calculate components scores.


\subsection{Factor Analysis}
Factor analysis is a statistical procedure to identify interrelationships that
exist among a large number of variables, i.e.,  to identify how suites of
variables are related.

Factor analysis can be used for exploratory or confirmatory purposes.
As an exploratory procedure, factor analysis is used to search for a
possible underlying structure in the variables. In confirmatory research,
the researcher evaluates how similar the actual structure of the data, as
indicated by factor analysis, is to the expected structure.

The major difference between exploratory and confirmatory factor
analysis is that researcher has formulated hypotheses about the
underlying structure of the variables when using factor analysis for
confirmatory purposes.

As an exploratory tool, factor analysis doesn't have many statistical
assumptions. The only real assumption is presence of relatedness
between the variables as represented by the correlation coefficient. If
there are no correlations, then there is no underlying structure.

\subsection{Exploratory Factor Analysis (EFA)}
\begin{itemize}
\item Is a variable reduction technique which identifies the number of \textbf{\emph{latent constructs}} and the underlying factor
structure of a set of variables
\item Hypothesizes an underlying construct, a variable not measured directly
\item Estimates factors which influence responses on observed variables
\item Allows you to describe and identify the number of latent constructs (also known as factors)
\item Includes unique factors, error due to unreliability in measurement
\item Traditionally has been used to explore the possible underlying factor structure of a set of measured variables without imposing any preconceived structure on the outcome.
\end{itemize}

The figure below shows four \textbf{\emph{factors}} (ovals) each measured by 3 observed variables (rectangles) with unique factors. Since measurement is not perfect, error or unreliability is estimated and specified explicitly in the diagram.

\textbf{\emph{Factor loadings}} (parameter estimates) help interpret factors. Loadings are the correlation between observed variables and factors, are standardized regression weights if variables are standardized (weights used to predict variables from factor). Standardized linear weights represent the effect size of the factor
on variability of observed variables.

\begin{figure}[h!]
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.6]{4AFactor.jpg}\\
  \caption{Factor Analysis}
\end{center}
\end{figure}

In Exploratory Factor Analysis, observed variables are a linear combination of the underlying factors (estimated factor and a unique factor).

Communality is the variance of observed variables accounted for by a common factor. Large communality is strongly
influenced by an underlying construct.

\subsection{Similarities between PCA and EFA}
\begin{itemize}
\item PCA and EFA have these assumptions in common:
\begin{itemize}
\item Measurement scale is interval or ratio level
\item Random sample - at least 5 observations per observed variable and at least 100 observations.
\item Larger sample sizes recommended for more stable estimates, 10-20 observations per observed variable
\end{itemize}
\item `Over-sample' to compensate for missing values
\item Linear relationship between observed variables
\item Normal distribution for each observed variable
\item Each pair of observed variables has a bivariate normal distribution
\item PCA and EFA are both variable reduction techniques. (If communalities are large, close to 1.00, results could be similar.)
\end{itemize}

\subsection{Differences between PCA and FA}
These techniques are typically used to analyze groups of correlated variables representing one or more common domains; for example, indicators of socioeconomic status, job satisfaction, health, self-esteem, political attitudes or family values. 
Principal components analysis is used to find optimal ways of combining variables into a small number of subsets, while factor analysis may be used to identify the structure underlying such variables and to estimate scores to measure latent factors themselves. The main applications of these techniques can be found in the analysis of multiple indicators, measurement and validation of complex constructs, index and scale construction, and data reduction. These approaches are particularly useful in situations where the dimensionality of data and its structural composition are not well known.

When an investigator has a set of hypotheses that form the conceptual basis for her/his factor analysis, the investigator performs a confirmatory, or hypothesis testing, factor analysis. In contrast, when there are no guiding hypotheses, when the question is simply what are the underlying factors the investigator conducts an exploratory factor analysis. 

The factors in factor analysis are conceptualized as "real world" entities such as depression, anxiety, and disturbed thought. This is in contrast to principal components analysis (PCA), where the components are simply geometrical abstractions that may not map easily onto real world phenomena.

\subsubsection{ Treatment of Variance}

Another difference between the two approaches has to do with the variance that is analyzed. In PCA, all of the observed variance is analyzed, while in factor analysis it is only the shared variances that is analyzed.
\begin{itemize}
\item Principal Components retained account for a maximal amount of variance of observed variables.
Factors account for common variance in the data.

\item PCA Analysis decomposes correlation matrix. EFA  decomposes adjusted correlation matrix.

\item PCA: Ones on the diagonals of the correlation matrix. EFA Diagonals of correlation matrix adjusted with unique factors.

\item PCA: Minimizes sum of squared perpendicular distance to
the component axis. EFA: Estimates factors which influence responses on
observed variables.
\item PCA: Component scores are a linear combination of the
observed variables weighted by eigenvectors.
EFA: Observed variables are linear combinations of the
underlying and unique factors.
\end{itemize}


%---------------------------------------------------------------------------------------------%
\newpage

\subsection{PCA Terminology}
\begin{itemize}
\item  PC loadings are correlation coefficients between the PC scores and the
original variables.
\item  PC loadings measure the importance of each variable in accounting for the
variability in the PC.  It is possible to interpret the first few PCs in terms of
'overall' effect or a 'contrast' between groups of variables based on the
structures of PC loadings.
\item high correlation between PC1 and a variable indicates that the variable is
associated with the direction of the maximum amount of variation in the
dataset.
\item More than one variable might have a high correlation with PC1. A strong
correlation between a variable and PC2 indicates that the variable is
responsible for the next largest variation in the data perpendicular to PC1,
and so on.
\item  if a variable does not correlate to any PC, or correlates only with the last PC,
or one before the last PC, this usually suggests that the variable has little or
no contribution to the variation in the dataset. Therefore, PCA may often
indicate which variables in a dataset are important and which ones may be of
little consequence. Some of these low-performance variables might
therefore be removed from consideration in order to simplify the overall
analyses.
\end{itemize}

\subsection{Bi-plot Display of PCA}
\begin{itemize}
\item Bi-plot display is a visualization technique for investigating
the inter-relationships between the observations and
variables in multivariate data.
\item  To display a bi-plot, the data should be considered as a
matrix, in which the column represents the variable space
while the row represents the observational space.
\item  The term bi-plot means it is a plot of two dimensions with the
observation and variable spaces plotted simultaneously.
\item  In PCA, relationships between PC scores and PCA loadings
associated with any two PCs can be illustrated in a bi-plot
display
\end{itemize}


\subsection{Communality} 
Communality refers to the total amount of variance an original variable shares with all other
variables included in the analysis.This is the proportion of each variable's variance that can be explained by the principal components .  (It is denoted as $h^2$ and can be defined as the sum of squared factor loadings).

\textbf{Initial} - By definition, the initial value of the communality in a principal components analysis is 1.

\textbf{Extraction}  - The values in this column indicate the proportion of each variable's variance that can be explained by the principal components.  Variables with high values are well represented in the common factor space, while variables with low values are not well represented. They are the reproduced variances from the number of components that you have saved.  You can find these values on the diagonal of the reproduced correlation matrix.


\subsection{What is a Rotation}

Ideally, you would like to review the correlations between the variables and the
components and use this information to interpret the components; that is, to determine what
construct seems to be measured by component 1, what construct seems to be measured by
component 2, and so forth. Unfortunately, when more than one component has been retained in
an analysis, the interpretation of an unrotated factor pattern is usually quite difficult. To make
interpretation easier, you will normally perform an operation called a rotation. A rotation is a
linear transformation that is performed on the factor solution for the purpose of making the
solution easier to interpret.
%-------------------------------------------------------------------------------------------------------%

\subsection{Varimax Rotation}
A varimax rotation is an orthogonal rotation, meaning that
it results in uncorrelated components. Compared to some other types of rotations, a varimax
rotation tends to maximize the variance of a column of the factor pattern matrix (as opposed to a
row of the matrix). This rotation is probably the most commonly used orthogonal rotation in the
social sciences.

\subsection{Interpreting the Rotated Solution}

Interpreting a rotated solution means determining just what is measured by each of the retained
components. Briefly, this involves identifying the variables that demonstrate high loadings for a
given component, and determining what these variables have in common. Usually, a brief name
is assigned to each retained component that describes its content.
The first decision to be made at this stage is to decide how large a factor loading must be to be
considered ``large."

Guidelines are provided in statistical literature for testing the statistical significance of factor loadings. Given that this is an introductory treatment of principal component analysis, however, simply consider a loading
to be large if its absolute value exceeds 0.40.


\end{document}

%-------------------------------------------------------------------------------------------------------%
%\subsection{Factor loadings}
%Factor loadings refer to the Correlations between the original variables and the factors, and
%the key to understanding the underlying nature of a particular factor. Squared
%factor loadings indicate what percentage of the variance in an original variable is
%explained by a factor.
%
%
%
%Factor loadings (factor or component coefficients) : The
%factor loadings, also called component loadings in PCA, are the
%correlation coefficients between the variables (rows) and
%factors (columns).
%
%
%
%Analogous to Pearson's r, the squared factor loading is the
%percent of variance in that variable explained by the factor.
%To get the percent of variance in all the variables accounted
%for by each factor, add the sum of the squared factor
%loadings for that factor (column) and divide by the number of
%variables. (Note the number of variables equals the sum of
%their variances as the variance of a standardized variable is
%1.) This is the same as dividing the factor's eigenvalue by the
%number of variables

\end{document} 